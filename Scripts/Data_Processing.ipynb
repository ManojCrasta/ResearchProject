{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e872f-2a22-4e2f-a3cc-0bb9c85f0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to clean and prepare data\n",
    "def clean_and_prepare_data(input_folder, output_file):\n",
    "    data = []\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as file:\n",
    "                content = json.load(file)\n",
    "                articles = content.get(\"articles\", [])\n",
    "                for article in articles:\n",
    "                    pub_date = article.get(\"pubDate\")\n",
    "                    description = article.get(\"description\", \"\")\n",
    "                    if pub_date:\n",
    "                        try:\n",
    "                            dt = datetime.fromisoformat(pub_date.replace(\"Z\", \"+00:00\"))\n",
    "                            rounded_time = dt.replace(minute=0, second=0, microsecond=0)\n",
    "                            formatted_date = rounded_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        except ValueError:\n",
    "                            formatted_date = None\n",
    "                    data.append({\"pub_date\": formatted_date, \"description\": description})\n",
    "    \n",
    "    df = pd.DataFrame(data).drop_duplicates(subset=['pub_date', 'description'])\n",
    "    df.replace([\"\", \"NULL\", \"N/A\"], pd.NA, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    # Check the number of rows after removing duplicates\n",
    "    final_row_count = df.shape[0]\n",
    "    print(f\"Number of rows after removing duplicates: {final_row_count}\")\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "\n",
    "\n",
    "def process_xauusd_data(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Processes the XAU/USD hourly data by combining DATE and TIME into a TIMESTAMP column and saves the result.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file (str): Path to the input Excel file containing XAU/USD data.\n",
    "        output_file (str): Path to the output Excel file to save the processed data.\n",
    "    \"\"\"\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # Combine DATE and TIME columns into a new TIMESTAMP column\n",
    "    df['TIMESTAMP'] = pd.to_datetime(df['DATE'].astype(str) + ' ' + df['TIME'].astype(str), format='%Y.%m.%d %H:%M:%S')\n",
    "\n",
    "    # Drop the original DATE and TIME columns\n",
    "    df.drop(['DATE', 'TIME', 'TICKVOL', 'VOL'], axis=1, inplace=True)\n",
    "\n",
    "    # Reorder columns to place TIMESTAMP at the beginning\n",
    "    df = df[['TIMESTAMP', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'SPREAD']]\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    # Save the result to a new Excel file\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Processed data with TIMESTAMP has been saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "def merge_xauusd_and_sentiment(price_file: str, sentiment_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Merges the XAU/USD price data with sentiment scores based on the TIMESTAMP and date columns, then saves the result.\n",
    "    \n",
    "    Parameters:\n",
    "        price_file (str): Path to the input Excel file containing the XAU/USD price data.\n",
    "        sentiment_file (str): Path to the input Excel file containing sentiment data.\n",
    "        output_file (str): Path to the output Excel file to save the merged data.\n",
    "    \"\"\"\n",
    "    # Check if the input files exist\n",
    "    if not os.path.exists(price_file):\n",
    "        raise FileNotFoundError(f\"Price file not found: {price_file}\")\n",
    "    if not os.path.exists(sentiment_file):\n",
    "        raise FileNotFoundError(f\"Sentiment file not found: {sentiment_file}\")\n",
    "\n",
    "    # Load the two Excel files\n",
    "    price_data = pd.read_excel(price_file)\n",
    "    sentiment_data = pd.read_excel(sentiment_file)\n",
    "\n",
    "    # Convert TIMESTAMP and date columns to datetime for proper merging\n",
    "    price_data['TIMESTAMP'] = pd.to_datetime(price_data['TIMESTAMP'])\n",
    "    sentiment_data['date'] = pd.to_datetime(sentiment_data['date'])\n",
    "\n",
    "    # Perform the left join to merge sentiment scores into price data\n",
    "    merged_data = price_data.merge(sentiment_data, how='left', left_on='TIMESTAMP', right_on='date')\n",
    "\n",
    "    # Drop the extra 'date' column as we only need TIMESTAMP in the final result\n",
    "    merged_data = merged_data.drop(columns=['date'])\n",
    "\n",
    "    merged_data['Sentiment Score'] = merged_data['Sentiment Score'].fillna(method='ffill')\n",
    "    merged_data['Sentiment Score'] = merged_data['Sentiment Score'].fillna(0)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    # Save the merged data back to a new Excel file\n",
    "    merged_data.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Merged data saved to {output_file}\")\n",
    "\n",
    "def process_crude_oil_data(input_file: str, output_file: str, start_date: str, end_date: str):\n",
    "    \"\"\"\n",
    "    Processes crude oil hourly data by combining date and time into a datetime column, filtering by date range, and saves the result.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file containing crude oil data.\n",
    "        output_file (str): Path to the output Excel file to save the filtered data.\n",
    "        start_date (str): Start date for filtering data in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date for filtering data in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "    # Load the CSV file, specifying the delimiter and column names\n",
    "    data = pd.read_csv(input_file, delimiter=';', names=['date', 'time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "\n",
    "    # Combine date and time columns into a single datetime column\n",
    "    data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    # Drop the original date, time, and unnecessary columns, keeping only datetime and close\n",
    "    data = data[['datetime', 'close']]\n",
    "\n",
    "    # Filter data for the specified date range\n",
    "    filtered_data = data[(data['datetime'] >= start_date) & (data['datetime'] <= end_date)]\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    # Save the filtered data to a new Excel file\n",
    "    filtered_data.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Filtered crude oil data saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_vix_data(input_file: str, output_file: str, start_date: str, end_date: str):\n",
    "    \"\"\"\n",
    "    Processes VIX hourly data by combining date and time into a datetime column, filtering by date range, and saves the result.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file containing VIX data.\n",
    "        output_file (str): Path to the output Excel file to save the filtered data.\n",
    "        start_date (str): Start date for filtering data in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date for filtering data in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "    # Load the CSV file, specifying the delimiter and column names\n",
    "    data = pd.read_csv(input_file, delimiter=';', names=['date', 'time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "\n",
    "    # Combine date and time columns into a single datetime column\n",
    "    data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    # Drop the original date, time, and unnecessary columns, keeping only datetime and close\n",
    "    data = data[['datetime', 'close']]\n",
    "\n",
    "    # Filter data for the specified date range\n",
    "    filtered_data = data[(data['datetime'] >= start_date) & (data['datetime'] <= end_date)]\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    # Save the filtered data to a new Excel file\n",
    "    filtered_data.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Filtered VIX data saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_sp500_data(input_file: str, output_file: str, start_date: str, end_date: str):\n",
    "    \"\"\"\n",
    "    Processes S&P 500 hourly data by combining date and time into a datetime column, filtering by date range, and saves the result.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file containing S&P 500 data.\n",
    "        output_file (str): Path to the output Excel file to save the filtered data.\n",
    "        start_date (str): Start date for filtering data in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date for filtering data in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "    # Load the CSV file, specifying the delimiter and column names\n",
    "    data = pd.read_csv(input_file, delimiter=';', names=['date', 'time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "\n",
    "    # Combine date and time columns into a single datetime column\n",
    "    data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    # Drop the original date, time, and unnecessary columns, keeping only datetime and close\n",
    "    data = data[['datetime', 'close']]\n",
    "\n",
    "    # Filter data for the specified date range\n",
    "    filtered_data = data[(data['datetime'] >= start_date) & (data['datetime'] <= end_date)]\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    # Save the filtered data to a new Excel file\n",
    "    filtered_data.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Filtered S&P 500 data saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_datasets(crude_file: str, xau_usd_file: str, vix_file: str, sp_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Merges multiple datasets (XAU/USD, crude oil, VIX, S&P 500) based on the 'datetime' column and saves the final merged dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        crude_file (str): Path to the input Excel file containing crude oil data.\n",
    "        xau_usd_file (str): Path to the input Excel file containing XAU/USD data.\n",
    "        vix_file (str): Path to the input Excel file containing VIX data.\n",
    "        sp_file (str): Path to the input Excel file containing S&P 500 data.\n",
    "        output_file (str): Path to the output Excel file to save the merged data.\n",
    "    \"\"\"\n",
    "    # Load the crude oil data\n",
    "    crude_data = pd.read_excel(crude_file, parse_dates=['datetime'])\n",
    "    crude_data.rename(columns={'close': 'crude_close'}, inplace=True)\n",
    "\n",
    "    # Load the XAU/USD data\n",
    "    xau_usd_data = pd.read_excel(xau_usd_file)\n",
    "    xau_usd_data['datetime'] = pd.to_datetime(xau_usd_data['TIMESTAMP'])\n",
    "\n",
    "    # Merge XAU/USD data with crude oil data on 'datetime'\n",
    "    merged_data = pd.merge(xau_usd_data, crude_data[['datetime', 'crude_close']], on='datetime', how='inner')\n",
    "    merged_data = merged_data.drop('TIMESTAMP', axis=1)\n",
    "\n",
    "    # Load the VIX data\n",
    "    vix_data = pd.read_excel(vix_file, parse_dates=['datetime'])\n",
    "    vix_data.rename(columns={'close': 'vix_close'}, inplace=True)\n",
    "\n",
    "    # Merge the VIX data with the previous merged data on 'datetime'\n",
    "    merged_data = pd.merge(merged_data, vix_data[['datetime', 'vix_close']], on='datetime', how='inner')\n",
    "\n",
    "    # Load the S&P 500 data\n",
    "    sp_data = pd.read_excel(sp_file, parse_dates=['datetime'])\n",
    "    sp_data.rename(columns={'close': 'sp_close'}, inplace=True)\n",
    "\n",
    "    # Merge the S&P 500 data with the merged data on 'datetime'\n",
    "    final_merged_data = pd.merge(merged_data, sp_data[['datetime', 'sp_close']], on='datetime', how='inner')\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    # Save the final merged dataset to a new file\n",
    "    final_merged_data.to_excel(output_file, index=False)\n",
    "\n",
    "    # Print the first few rows of the final merged data\n",
    "    print(final_merged_data.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
